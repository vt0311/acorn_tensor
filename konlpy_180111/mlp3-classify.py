from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import train_test_split
from sklearn import model_selection, metrics
import json

max_words = 67395 # 입력 단어 수: word-dic.json 파일 참고
max_words = 56681 # 입력 단어 수: word-dic.json 파일 참고
nb_classes = 9    # 9개의 카테고리
batch_size = 64 
nb_epoch = 20

# MLP 모델 생성하기 --- (※1)
def build_model():
    model = Sequential()
    model.add(Dense(512, input_shape=(max_words,)))
    model.add(Activation('relu'))
    model.add(Dropout(0.5))
    model.add(Dense(nb_classes))
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy',
        optimizer='adam',
        metrics=['accuracy'])
    return model

# 데이터 읽어 들이기--- (※2)
# data = json.load(open("./newstext/data-mini.json")) 
data = json.load(open("./newstext/data.json"))

X = data["X"] # 텍스트를 나타내는 데이터
Y = data["Y"] # 카테고리 데이터

# 학습하기 --- (※3)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y)
Y_train = np_utils.to_categorical(Y_train, nb_classes)

print(len(X_train),len(Y_train))

model = KerasClassifier(
    build_fn=build_model, 
    nb_epoch=nb_epoch, 
    batch_size=batch_size)

model.fit(X_train, Y_train)
# 예측하기 --- (※4)
y = model.predict(X_test)
ac_score = metrics.accuracy_score(Y_test, y)
cl_report = metrics.classification_report(Y_test, y)

print("정답률 =", ac_score)
print("리포트 =\n", cl_report)

'''
data-mini.json으로 테스트시 결과

Using TensorFlow backend.
99 99
Epoch 1/1
2018-01-11 00:16:44.013600: I C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\35\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2

64/99 [==================>...........] - ETA: 0s - loss: 2.2389 - acc: 0.1562
99/99 [==============================] - 2s 20ms/step - loss: 2.0614 - acc: 0.2525
정답률 = 0.575757575758
리포트 =
              precision    recall  f1-score   support

          0       0.88      1.00      0.93         7
          1       1.00      0.14      0.25         7
          2       1.00      0.12      0.22         8
          3       0.29      1.00      0.44         2
          4       0.43      1.00      0.60         3
          5       0.56      0.83      0.67         6

avg / total       0.80      0.58      0.51        33

'''


'''
data.json으로 테스트시 결과

Using TensorFlow backend.
6314 6314
Epoch 1/1
2018-01-11 13:15:39.676475: I C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\
36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instruct
ions that this TensorFlow binary was not compiled to use: AVX AVX2

  64/6314 [..............................] - ETA: 5:42:09 - loss: 2.3705 - acc: 
0.0625
 128/6314 [..............................] - ETA: 2:54:41 - loss: 2.0341 - acc: 
0.2891C:\Users\acorn\AppData\Local\Programs\Python\Python36\lib\site-packages\ke
ras\callbacks.py:116: UserWarning: Method on_batch_end() is slow compared to the
 batch update (2.179625). Check your callbacks.
  % delta_t_median)

 192/6314 [..............................] - ETA: 1:59:10 - loss: 1.8043 - acc: 
0.4062
 256/6314 [>.............................] - ETA: 1:29:11 - loss: 1.6788 - acc: 
0.4766
 320/6314 [>.............................] - ETA: 1:11:27 - loss: 1.5473 - acc: 
0.5250
 384/6314 [>.............................] - ETA: 59:36 - loss: 1.4463 - acc: 0.
5599  
 448/6314 [=>............................] - ETA: 51:05 - loss: 1.3507 - acc: 0.
5915
 512/6314 [=>............................] - ETA: 44:31 - loss: 1.2847 - acc: 0.
6133
 576/6314 [=>............................] - ETA: 39:49 - loss: 1.2285 - acc: 0.
6302
 640/6314 [==>...........................] - ETA: 35:50 - loss: 1.2066 - acc: 0.
6422
 704/6314 [==>...........................] - ETA: 32:27 - loss: 1.1500 - acc: 0.
6634
 768/6314 [==>...........................] - ETA: 29:40 - loss: 1.1241 - acc: 0.
6758
 832/6314 [==>...........................] - ETA: 27:14 - loss: 1.0951 - acc: 0.
6863
 896/6314 [===>..........................] - ETA: 25:11 - loss: 1.0660 - acc: 0.
6942
 960/6314 [===>..........................] - ETA: 23:25 - loss: 1.0470 - acc: 0.
7021
1024/6314 [===>..........................] - ETA: 21:52 - loss: 1.0246 - acc: 0.
7109
1088/6314 [====>.........................] - ETA: 20:38 - loss: 0.9945 - acc: 0.
7178
1152/6314 [====>.........................] - ETA: 19:27 - loss: 0.9639 - acc: 0.
7283
1216/6314 [====>.........................] - ETA: 18:22 - loss: 0.9516 - acc: 0.
7327
1280/6314 [=====>........................] - ETA: 17:23 - loss: 0.9386 - acc: 0.
7375
1344/6314 [=====>........................] - ETA: 16:27 - loss: 0.9383 - acc: 0.
7426
1408/6314 [=====>........................] - ETA: 15:39 - loss: 0.9318 - acc: 0.
7464
1472/6314 [=====>........................] - ETA: 14:51 - loss: 0.9087 - acc: 0.
7500
1536/6314 [======>.......................] - ETA: 14:09 - loss: 0.8930 - acc: 0.
7559
1600/6314 [======>.......................] - ETA: 13:33 - loss: 0.8767 - acc: 0.
7588
1664/6314 [======>.......................] - ETA: 12:57 - loss: 0.8845 - acc: 0.
7578
1728/6314 [=======>......................] - ETA: 12:23 - loss: 0.8740 - acc: 0.
7616
1792/6314 [=======>......................] - ETA: 11:52 - loss: 0.8550 - acc: 0.
7656
1856/6314 [=======>......................] - ETA: 11:23 - loss: 0.8465 - acc: 0.
7689
1920/6314 [========>.....................] - ETA: 10:56 - loss: 0.8390 - acc: 0.
7708
1984/6314 [========>.....................] - ETA: 10:34 - loss: 0.8314 - acc: 0.
7717
2048/6314 [========>.....................] - ETA: 10:10 - loss: 0.8161 - acc: 0.
7754
2112/6314 [=========>....................] - ETA: 9:48 - loss: 0.8133 - acc: 0.7
756 
2176/6314 [=========>....................] - ETA: 9:27 - loss: 0.8016 - acc: 0.7
785
2240/6314 [=========>....................] - ETA: 9:09 - loss: 0.7831 - acc: 0.7
844
2304/6314 [=========>....................] - ETA: 8:49 - loss: 0.7804 - acc: 0.7
869
2368/6314 [==========>...................] - ETA: 8:29 - loss: 0.7708 - acc: 0.7
884
2432/6314 [==========>...................] - ETA: 8:12 - loss: 0.7566 - acc: 0.7
924
2496/6314 [==========>...................] - ETA: 7:56 - loss: 0.7501 - acc: 0.7
929
2560/6314 [===========>..................] - ETA: 7:39 - loss: 0.7433 - acc: 0.7
937
2624/6314 [===========>..................] - ETA: 7:27 - loss: 0.7376 - acc: 0.7
961
2688/6314 [===========>..................] - ETA: 7:11 - loss: 0.7328 - acc: 0.7
976
2752/6314 [============>.................] - ETA: 6:58 - loss: 0.7287 - acc: 0.7
991
2816/6314 [============>.................] - ETA: 6:46 - loss: 0.7239 - acc: 0.8
018
2880/6314 [============>.................] - ETA: 6:35 - loss: 0.7178 - acc: 0.8
028
2944/6314 [============>.................] - ETA: 6:22 - loss: 0.7152 - acc: 0.8
023
3008/6314 [=============>................] - ETA: 6:09 - loss: 0.7049 - acc: 0.8
052
3072/6314 [=============>................] - ETA: 5:59 - loss: 0.6973 - acc: 0.8
060
3136/6314 [=============>................] - ETA: 5:47 - loss: 0.6961 - acc: 0.8
080
3200/6314 [==============>...............] - ETA: 5:36 - loss: 0.6914 - acc: 0.8
091
3264/6314 [==============>...............] - ETA: 5:25 - loss: 0.6906 - acc: 0.8
091
3328/6314 [==============>...............] - ETA: 5:16 - loss: 0.6878 - acc: 0.8
098
3392/6314 [===============>..............] - ETA: 5:07 - loss: 0.6841 - acc: 0.8
107
3456/6314 [===============>..............] - ETA: 4:58 - loss: 0.6782 - acc: 0.8
125
3520/6314 [===============>..............] - ETA: 4:48 - loss: 0.6710 - acc: 0.8
139
3584/6314 [================>.............] - ETA: 4:39 - loss: 0.6659 - acc: 0.8
153
3648/6314 [================>.............] - ETA: 4:29 - loss: 0.6663 - acc: 0.8
161
3712/6314 [================>.............] - ETA: 4:20 - loss: 0.6614 - acc: 0.8
173
3776/6314 [================>.............] - ETA: 4:10 - loss: 0.6570 - acc: 0.8
181
3840/6314 [=================>............] - ETA: 4:01 - loss: 0.6536 - acc: 0.8
193
3904/6314 [=================>............] - ETA: 3:53 - loss: 0.6506 - acc: 0.8
210
3968/6314 [=================>............] - ETA: 3:44 - loss: 0.6424 - acc: 0.8
233
4032/6314 [==================>...........] - ETA: 3:36 - loss: 0.6361 - acc: 0.8
254
4096/6314 [==================>...........] - ETA: 3:28 - loss: 0.6320 - acc: 0.8
267
4160/6314 [==================>...........] - ETA: 3:20 - loss: 0.6293 - acc: 0.8
272
4224/6314 [===================>..........] - ETA: 3:13 - loss: 0.6318 - acc: 0.8
277
4288/6314 [===================>..........] - ETA: 3:05 - loss: 0.6288 - acc: 0.8
286
4352/6314 [===================>..........] - ETA: 2:58 - loss: 0.6244 - acc: 0.8
288
4416/6314 [===================>..........] - ETA: 2:51 - loss: 0.6213 - acc: 0.8
299
4480/6314 [====================>.........] - ETA: 2:44 - loss: 0.6168 - acc: 0.8
317
4544/6314 [====================>.........] - ETA: 2:37 - loss: 0.6125 - acc: 0.8
330
4608/6314 [====================>.........] - ETA: 2:32 - loss: 0.6079 - acc: 0.8
342
4672/6314 [=====================>........] - ETA: 2:27 - loss: 0.6083 - acc: 0.8
348
4736/6314 [=====================>........] - ETA: 2:22 - loss: 0.6047 - acc: 0.8
353
4800/6314 [=====================>........] - ETA: 2:16 - loss: 0.6024 - acc: 0.8
360
4864/6314 [======================>.......] - ETA: 2:10 - loss: 0.5981 - acc: 0.8
376
4928/6314 [======================>.......] - ETA: 2:05 - loss: 0.5987 - acc: 0.8
383
4992/6314 [======================>.......] - ETA: 2:00 - loss: 0.5939 - acc: 0.8
393
5056/6314 [=======================>......] - ETA: 1:54 - loss: 0.5914 - acc: 0.8
406
5120/6314 [=======================>......] - ETA: 1:48 - loss: 0.5875 - acc: 0.8
414
5184/6314 [=======================>......] - ETA: 1:42 - loss: 0.5843 - acc: 0.8
418
5248/6314 [=======================>......] - ETA: 1:36 - loss: 0.5840 - acc: 0.8
422
5312/6314 [========================>.....] - ETA: 1:30 - loss: 0.5808 - acc: 0.8
428
5376/6314 [========================>.....] - ETA: 1:24 - loss: 0.5796 - acc: 0.8
439
5440/6314 [========================>.....] - ETA: 1:18 - loss: 0.5754 - acc: 0.8
450
5504/6314 [=========================>....] - ETA: 1:12 - loss: 0.5736 - acc: 0.8
454
5568/6314 [=========================>....] - ETA: 1:06 - loss: 0.5741 - acc: 0.8
450
5632/6314 [=========================>....] - ETA: 1:00 - loss: 0.5729 - acc: 0.8
452
5696/6314 [==========================>...] - ETA: 54s - loss: 0.5699 - acc: 0.84
64 
5760/6314 [==========================>...] - ETA: 48s - loss: 0.5678 - acc: 0.84
74
5824/6314 [==========================>...] - ETA: 42s - loss: 0.5682 - acc: 0.84
79
5888/6314 [==========================>...] - ETA: 37s - loss: 0.5653 - acc: 0.84
82
5952/6314 [===========================>..] - ETA: 31s - loss: 0.5632 - acc: 0.84
88
6016/6314 [===========================>..] - ETA: 25s - loss: 0.5596 - acc: 0.84
97
6080/6314 [===========================>..] - ETA: 20s - loss: 0.5624 - acc: 0.85
00
6144/6314 [============================>.] - ETA: 14s - loss: 0.5602 - acc: 0.85
03
6208/6314 [============================>.] - ETA: 9s - loss: 0.5582 - acc: 0.851
2 
6272/6314 [============================>.] - ETA: 3s - loss: 0.5562 - acc: 0.851
7
6314/6314 [==============================] - 541s 86ms/step - loss: 0.5558 - acc
: 0.8518
정답률 = 0.901662707838
리포트 =
              precision    recall  f1-score   support

          0       0.96      0.86      0.91       353
          1       0.93      0.92      0.93       342
          2       0.84      0.91      0.87       379
          3       0.86      0.86      0.86       330
          4       0.91      0.92      0.91       344
          5       0.92      0.94      0.93       357

avg / total       0.90      0.90      0.90      2105

'''